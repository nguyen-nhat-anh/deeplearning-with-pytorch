{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Các hàm (đặc biệt trong neural network) dù phức tạp nhưng cũng chỉ là hợp của các phép toán/hàm cơ bản (cộng, trừ, nhân, chia, sin, cos, exp, log, v.v.). Các phép toán/hàm cơ bản ấy đều có công thức đạo hàm $\\rightarrow$ áp dụng chain rule là có thể tính được đạo hàm của bất kì hợp các hàm cơ bản.\n",
    "\n",
    "* **Computational graph**: biểu diễn hàm hợp phức tạp ở trên bằng đồ thị. Các node là các biến/tensor, các mũi tên biểu diễn các phép toán/hàm cơ bản.\n",
    "\n",
    "* **Automatic differentiation (autodiff)**: kĩ thuật tính đạo hàm bằng chain rule dựa vào computational graph. Có 2 kĩ thuật: forward mode autodiff và reverse mode autodiff. Ở đây ta chỉ quan tâm reverse mode autodiff (backpropagation trong deep learning).\n",
    "\n",
    "Ví dụ: \n",
    "$$y = (x - 1)^2 + (x + 1)^2$$\n",
    "![autodiff](resources/autodiff.png)\n",
    "\\begin{align}\n",
    "a &= x - 1 \\\\\n",
    "b &= x + 1 \\\\\n",
    "c &= a^2 \\\\\n",
    "d &= b^2 \\\\\n",
    "y &= c + d\n",
    "\\end{align}\n",
    "\n",
    "Tính $\\dfrac{\\partial y}{\\partial x}$ theo chain rule\n",
    "\\begin{align}\n",
    "\\dfrac{\\partial y}{\\partial x} &= \\text{tổng các đường từ x $\\rightarrow$ y} \\\\\n",
    "&= \\underbrace{\\dfrac{\\partial y}{\\partial c}\\dfrac{\\partial c}{\\partial a}}_{\\dfrac{\\partial y}{\\partial a}}\\dfrac{\\partial a}{\\partial x} + \\underbrace{\\dfrac{\\partial y}{\\partial d}\\dfrac{\\partial d}{\\partial b}}_{\\dfrac{\\partial y}{\\partial b}}\\dfrac{\\partial b}{\\partial x}\n",
    "\\end{align}\n",
    "\n",
    "Kí hiệu $\\dfrac{\\partial y}{\\partial x}$ là $\\overline{x}$ (tương tự $\\dfrac{\\partial y}{\\partial a}$ là $\\overline{a}$, $\\dfrac{\\partial y}{\\partial b}$ là $\\overline{b}, \\ldots$) thì\n",
    "\n",
    "$$\\overline{x} = \\overline{a} \\cdot \\dfrac{\\partial a}{\\partial x} + \\overline{b} \\cdot \\dfrac{\\partial b}{\\partial x}$$\n",
    "$\\dfrac{\\partial a}{\\partial x}$ và $\\dfrac{\\partial b}{\\partial x}$ đều là đạo hàm của các phép toán cơ bản nên tính được đơn giản. Vì vậy nếu biết $\\overline{a}$ và $\\overline{b}$ (để ý $a$ và $b$ đều là child node của $x$) thì sẽ tính được $\\overline{x}$. $\\overline{a}$ tính được nếu biết $\\overline{c}$, $\\overline{b}$ tính được nếu biết $\\overline{d}$. Mà $\\overline{c}$ và $\\overline{d}$ cũng tính được đơn giản vì đều là đạo hàm của phép toán cơ bản $\\rightarrow$ ý tưởng reverse mode autodiff.\n",
    "![autodiff2](resources/autodiff2.png)\n",
    "Chi tiết các bước tính toán:\n",
    "1. Tính $\\overline{c}$ và $\\overline{d}$\n",
    "\\begin{align}\n",
    "\\overline{c} &= \\dfrac{\\partial y}{\\partial c} = \\dfrac{\\partial}{\\partial c} (c + d) = 1 \\\\\n",
    "\\overline{d} &= \\dfrac{\\partial y}{\\partial d} = \\dfrac{\\partial}{\\partial d} (c + d) = 1\n",
    "\\end{align}\n",
    "2. Tính $\\overline{a}$ và $\\overline{b}$\n",
    "\\begin{align}\n",
    "\\overline{a} &= \\overline{c} \\cdot \\dfrac{\\partial c}{\\partial a} = 1 \\cdot \\dfrac{\\partial}{\\partial a} a^2 = 2a \\\\\n",
    "\\overline{b} &= \\overline{d} \\cdot \\dfrac{\\partial d}{\\partial b} = 1 \\cdot \\dfrac{\\partial}{\\partial b} b^2 = 2b\n",
    "\\end{align}\n",
    "3. Tính $\\overline{x}$\n",
    "\\begin{align}\n",
    "\\overline{x} &= \\overline{a} \\cdot \\dfrac{\\partial a}{\\partial x} + \\overline{b} \\cdot \\dfrac{\\partial b}{\\partial x} \\\\\n",
    "&= 2a \\cdot 1 + 2b \\cdot 1 = 4x\n",
    "\\end{align}\n",
    "\n",
    "Tính đạo hàm với `torch.autograd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 4.5\n",
      "a = x - 1 = 3.5\n",
      "b = x + 1 = 5.5\n",
      "c = a^2 = 12.25\n",
      "d = b^2 = 30.25\n",
      "y = c + d = 42.5\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(4.5, requires_grad=True) # set requires_grad=True để mọi operation trên với tensor này đều được record\n",
    "a = x - 1\n",
    "b = x + 1\n",
    "c = a**2\n",
    "d = b**2\n",
    "y = c + d\n",
    "print(f'x = {x}')\n",
    "print(f'a = x - 1 = {a}')\n",
    "print(f'b = x + 1 = {b}')\n",
    "print(f'c = a^2 = {c}')\n",
    "print(f'd = b^2 = {d}')\n",
    "print(f'y = c + d = {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gọi `y.backward()` để tính đạo hàm của `y` theo \"graph leaves\" (ở đây là biến `x`)\n",
    "\n",
    "(**Note**: thông thường node `x` trong ví dụ này hay được gọi là root node, còn `y` mới gọi là leaf node. Có lẽ tutorial của pytorch cố tình đảo ngược tên gọi như thế vì trong context deeplearning thường thì chỉ có 1 output `y` trong khi có thể có nhiều biến input `x` $\\rightarrow$ gọi output là root và gọi các biến input là các leaves nghe trực quan hơn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward() # Computes the gradient of current tensor w.r.t. graph leaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đạo hàm của `y` theo `x` được lưu ở attribute `x.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx = 18.0\n"
     ]
    }
   ],
   "source": [
    "print(f'dy/dx = {x.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ví dụ 2: Tính đạo hàm của hàm sigmoid\n",
    "$$y = \\sigma(x) = \\dfrac{1}{1 + e^{-x}}$$\n",
    "![autodiff3](resources/autodiff3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 1.0\n",
      "y = sigmoid(1.0) = 0.7310585975646973\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = sigmoid(x)\n",
    "print(f'x = {x}')\n",
    "print(f'y = sigmoid({x}) = {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có thể kiểm chứng đạo hàm của hàm sigmoid $\\sigma(x)$ là $\\sigma(x)\\cdot(1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx = 0.1966119408607483\n",
      "y.(1 - y) = 0.1966119259595871\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(f'dy/dx = {x.grad}')\n",
    "print(f'y.(1 - y) = {y*(1 - y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ví dụ 3: nhiều biến đầu vào\n",
    "\\begin{align}\n",
    "a &= x + t \\\\\n",
    "b &= x - t \\\\\n",
    "y &= a \\cdot b = (x + t) \\cdot (x - t)\n",
    "\\end{align}\n",
    "Tính $\\overline{x} = \\dfrac{\\partial y}{\\partial x}$ và $\\overline{t} = \\dfrac{\\partial y}{\\partial t}$ \n",
    "![autodiff4](resources/autodiff4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 4.5\n",
      "t = 2.5\n",
      "a = x + t = 7.0\n",
      "b = x - t = 2.0\n",
      "y = a*b = 14.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(4.5, requires_grad=True)\n",
    "t = torch.tensor(2.5, requires_grad=True)\n",
    "a = x + t\n",
    "b = x - t\n",
    "y = a*b\n",
    "print(f'x = {x}')\n",
    "print(f't = {t}')\n",
    "print(f'a = x + t = {a}')\n",
    "print(f'b = x - t = {b}')\n",
    "print(f'y = a*b = {y}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx = 9.0\n",
      "dy/dt = -5.0\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(f'dy/dx = {x.grad}')\n",
    "print(f'dy/dt = {t.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khi các node là các tensor\n",
    "$y=f(x)$\n",
    "* Cả $x$ và $y$ đều là scalar $\\rightarrow$ đạo hàm cũng là scalar\n",
    "* $y$ scalar, $x$ vector $n$ chiều $\\rightarrow$ đạo hàm là vector gradient $n$ chiều\n",
    "* $y$ vector $m$ chiều, $x$ vector $n$ chiều $\\rightarrow$ đạo hàm là ma trận jacobian $m \\times n$\n",
    "\n",
    "Ví dụ: \n",
    "\\begin{align}\n",
    "\\mathbf{x} &= \n",
    "\\begin{bmatrix}\n",
    "x_1 & x_2 & x_3\n",
    "\\end{bmatrix} \\\\\n",
    "\\mathbf{y} &= \\mathbf{x}^2 = \n",
    "\\begin{bmatrix}\n",
    "x_1^2 & x_2^2 & x_3^2\n",
    "\\end{bmatrix} \\\\\n",
    "z &= sum(\\mathbf{y}) = y_1 + y_2 + y_3 = x_1^2 + x_2^2 + x_3^2\n",
    "\\end{align}\n",
    "![autodiff5](resources/autodiff5.png)\n",
    "\n",
    "* $\\overline{\\mathbf{y}} = \\dfrac{\\partial z}{\\partial \\mathbf{y}} = \n",
    "\\begin{bmatrix} \\overline{y}_1 & \\overline{y}_2 & \\overline{y}_3 \\end{bmatrix} =\n",
    "\\begin{bmatrix} \\dfrac{\\partial z}{\\partial y_1} & \\dfrac{\\partial z}{\\partial y_2} & \\dfrac{\\partial z}{\\partial y_3} \\end{bmatrix} = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}$\n",
    "* $\\overline{\\mathbf{x}} = \\overline{\\mathbf{y}} \\cdot \\dfrac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \\overline{\\mathbf{y}} \\cdot\n",
    "\\begin{bmatrix}\n",
    "\\dfrac{\\partial y_1}{\\partial x_1} & \\dfrac{\\partial y_1}{\\partial x_2} & \\dfrac{\\partial y_1}{\\partial x_3} \\\\\n",
    "\\dfrac{\\partial y_2}{\\partial x_1} & \\dfrac{\\partial y_2}{\\partial x_2} & \\dfrac{\\partial y_2}{\\partial x_3} \\\\\n",
    "\\dfrac{\\partial y_3}{\\partial x_1} & \\dfrac{\\partial y_3}{\\partial x_2} & \\dfrac{\\partial y_3}{\\partial x_3}\n",
    "\\end{bmatrix} = \\begin{bmatrix} \\overline{y}_1 & \\overline{y}_2 & \\overline{y}_3 \\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix} \n",
    "2x_1 & 0 & 0 \\\\\n",
    "0 & 2x_2 & 0 \\\\\n",
    "0 & 0 & 2x_3\n",
    "\\end{bmatrix} = \\begin{bmatrix} 2x_1 & 2x_2 &2 x_3 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1. 2. 3.]\n",
      "y = x^2 = [1. 4. 9.]\n",
      "z = sum(y) = 14.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x**2\n",
    "z = torch.sum(y)\n",
    "print(f'x = {x.detach().numpy()}')\n",
    "print(f'y = x^2 = {y.detach().numpy()}')\n",
    "print(f'z = sum(y) = {z}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx = [2. 4. 6.]\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(f'dz/dx = {x.grad.detach().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Có thể thấy khi thực hiện autodiff với node output là scalar, các node còn lại là các vector thường phải thực hiện phép nhân ma trận giữa vector gradient và ma trận Jacobian $\\mathbf{v} \\cdot \\mathbf{J}$ (gọi là vector-Jacobian product hay vjp) $\\rightarrow$ có cách nào để tính được vector-Jacobian product mà không cần thực hiện trực tiếp phép nhân ma trận?\n",
    "\n",
    "Trở lại ví dụ ở trên \n",
    "$\\overline{\\mathbf{x}} = \\overline{\\mathbf{y}} \\cdot \\dfrac{\\partial \\mathbf{y}}{\\partial \\mathbf{x}} = \n",
    "\\begin{bmatrix} \\overline{y}_1 & \\overline{y}_2 & \\overline{y}_3 \\end{bmatrix} \\cdot\n",
    "\\begin{bmatrix} \n",
    "2x_1 & 0 & 0 \\\\\n",
    "0 & 2x_2 & 0 \\\\\n",
    "0 & 0 & 2x_3\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix} \\overline{y}_1 \\cdot 2x_1 & \\overline{y}_2 \\cdot 2x_2 & \\overline{y}_3 \\cdot 2x_3 \\end{bmatrix} = 2\n",
    "\\underbrace{\\overline{\\mathbf{y}} \\odot \\mathbf{x}}_{\\text{element-wise product}}$\n",
    "\n",
    "Như vậy có thể tính $\\overline{\\mathbf{x}}$ bằng cách nhân element-wise giữa $\\overline{\\mathbf{y}}$ và $\\mathbf{x}$ (và nhân $2$ nữa) mà không cần phải trực tiếp thực hiện nhân ma trận.\n",
    "\\begin{align}\n",
    "\\text{vjp}_{(\\cdot)^2}(\\mathbf{v}, \\mathbf{y}, \\mathbf{x}) &= 2 \\mathbf{v} \\odot \\mathbf{x} \\\\\n",
    "\\overline{\\mathbf{x}} = \\text{vjp}_{(\\cdot)^2}(\\overline{\\mathbf{y}}, \\mathbf{y}, \\mathbf{x}) &= 2 \\overline{\\mathbf{y}} \\odot \\mathbf{x}\n",
    "\\end{align}\n",
    "\n",
    "Ở ví dụ trên, nếu muốn tính $\\text{vjp}_{(\\cdot)^2}$ với vector $\\mathbf{v}$ bất kì ta gọi `y.backward(gradient=v)` thay vì `z.backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1. 2. 3.]\n",
      "y = x^2 = [1. 4. 9.]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = x**2\n",
    "print(f'x = {x.detach().numpy()}')\n",
    "print(f'y = x^2 = {y.detach().numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v = [-1.  1.  2.]\n",
      "vjp_elementwise_square(v) = [-2.  4. 12.]\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([-1.0, 1.0, 2.0])\n",
    "y.backward(gradient=v)\n",
    "print(f'v = {v.numpy()}')\n",
    "print(f'vjp_elementwise_square(v) = {x.grad.detach().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tham khảo\n",
    "* https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/slides/lec10.pdf\n",
    "* https://github.com/mattjj/autodidact"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
